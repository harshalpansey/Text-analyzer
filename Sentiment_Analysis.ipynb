{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfmNimTFo5uE",
        "outputId": "a62729b7-eb1c-4fee-c975-256c4cd01877"
      },
      "source": [
        "import nltk \r\n",
        "nltk.download('twitter_samples')\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')  #for stopwords\r\n",
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger') # for speech Tagging\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')  # Database of words to find meaning of a token\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')   #converts text into list of sentences\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer   # for lemmatizzation of a word\r\n",
        "from nltk.corpus import twitter_samples, stopwords # for stopwords \r\n",
        "from nltk.tag import pos_tag                       # for tags contain noun or verb or adjective\r\n",
        "from nltk.tokenize import word_tokenize            # for tokenization\r\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier    #model\r\n",
        "import re, string, random"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqNVKlEcdt_Z"
      },
      "source": [
        "# Functions for Cleaning the data\r\n",
        "Tokenization, Removal of Stopwords, Lemmitization, \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43xLQE5VuzAd"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\r\n",
        "\r\n",
        "    cleaned_tokens = []\r\n",
        "\r\n",
        "    for token, tag in pos_tag(tweet_tokens):\r\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\r\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)   # removing stopwords\r\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\r\n",
        "\r\n",
        "        if tag.startswith(\"NN\"):                     #If a  token is NN then change its positional tag as n for noun\r\n",
        "            pos = 'n'\r\n",
        "        elif tag.startswith('VB'):\r\n",
        "            pos = 'v'\r\n",
        "        else:\r\n",
        "            pos = 'a'\r\n",
        "\r\n",
        "        lemmatizer = WordNetLemmatizer()            # for generating a root word of that token\r\n",
        "        token = lemmatizer.lemmatize(token, pos)\r\n",
        "\r\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\r\n",
        "            cleaned_tokens.append(token.lower())      # appending a cleaned token \r\n",
        "    return cleaned_tokens\r\n",
        "\r\n",
        "def get_all_words(cleaned_tokens_list):\r\n",
        "    for tokens in cleaned_tokens_list:\r\n",
        "        for token in tokens:\r\n",
        "            yield token                         # it returns a generator token without exiting a end value\r\n",
        "\r\n",
        "def get_tweets_for_model(cleaned_tokens_list):\r\n",
        "    for tweet_tokens in cleaned_tokens_list:\r\n",
        "        yield dict([token, True] for token in tweet_tokens)\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3fcao18dqNG"
      },
      "source": [
        "# Gathering the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2bYtwogdhnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ace174-26a2-48bf-9314-8ad1a9416dcd"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')   # positive tweets\r\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')   #negative tweets\r\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')       #textfor analysis \r\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0] # tokenize positive tweets\r\n",
        "\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "\r\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json') #Tokenize positive tweets \r\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json') #Tokenize negative tweets\r\n",
        "\r\n",
        "positive_cleaned_tokens_list = []\r\n",
        "negative_cleaned_tokens_list = []\r\n",
        "\r\n",
        "for tokens in positive_tweet_tokens:\r\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) #appends cleaned tokens without stopwords\r\n",
        "\r\n",
        "for tokens in negative_tweet_tokens:\r\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) #appends cleaned tokens without stopwords\r\n",
        "\r\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)               #running this function we get all positive words in positive sent list\r\n",
        "\r\n",
        "freq_dist_pos = FreqDist(all_pos_words)                                   # most occuring words in the list\r\n",
        "print(freq_dist_pos.most_common(10))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT3CikzSGaB3"
      },
      "source": [
        "# Preparing data for model\r\n",
        "Here we use naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of8XVe5RGXZB",
        "outputId": "7ed8c1e8-06f0-4c8c-be8a-12c55ee62e48"
      },
      "source": [
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\r\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\r\n",
        "\r\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\r\n",
        "                        for tweet_dict in positive_tokens_for_model]                #Converting Tokens into dictionary\r\n",
        "\r\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\r\n",
        "                        for tweet_dict in negative_tokens_for_model]                #Converting Tokens into dictionary\r\n",
        "\r\n",
        "dataset = positive_dataset + negative_dataset\r\n",
        "\r\n",
        "random.shuffle(dataset)                                                             #Shuffling the data for Test and Train\r\n",
        "\r\n",
        "train_data = dataset[:7000]\r\n",
        "test_data = dataset[7000:]\r\n",
        "\r\n",
        "classifier = NaiveBayesClassifier.train(train_data)\r\n",
        "\r\n",
        "print(\"Accuracy:- \", classify.accuracy(classifier, test_data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:-  0.996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgo3e637HUJU"
      },
      "source": [
        "# Taking custom input for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frg6uZfyk0Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9121b3-0a19-43b0-ac97-42472b5f851d"
      },
      "source": [
        "\r\n",
        "custom_tweet = input()\r\n",
        "\r\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\r\n",
        "\r\n",
        "print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I ordered just once from TerribleCo, they screwed up, never used the app again\n",
            "I ordered just once from TerribleCo, they screwed up, never used the app again Negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}